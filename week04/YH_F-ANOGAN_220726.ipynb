{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b64441c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BatchNormalization' from 'keras.layers.normalization' (C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\keras\\layers\\normalization\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 44>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvolutional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UpSampling2D, Conv2D, Conv2DTranspose\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpooling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaxPooling2D\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchNormalization\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madvanced_activations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LeakyReLU\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregularizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m l2\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BatchNormalization' from 'keras.layers.normalization' (C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\keras\\layers\\normalization\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "# %% [code]\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "# %% [code]\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from six.moves import range\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import Activation, ZeroPadding2D\n",
    "\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.generic_utils import Progbar\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "# %% [code]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %% [code]\n",
    "\n",
    "# TRAIN_IMG = os.listdir('/kaggle/input/steel-surface-defect-detection/train_images')\n",
    "# TEST_IMG = os.listdir('/kaggle/input/steel-surface-defect-detection/test_images')\n",
    "# print('Original train count =',len(TRAIN_IMG),', Original test count =',len(TEST_IMG))\n",
    "# print('New train count = 1801 , New test count = 1801')\n",
    "# os.mkdir('../tmp/')\n",
    "# os.mkdir('../tmp/train_images/')\n",
    "# r = np.random.choice(TRAIN_IMG,len(TEST_IMG),replace=False)\n",
    "# for i,f in enumerate(r):\n",
    "#     img = Image.open('../input/steel-surface-defect-detection/train_images/'+f)\n",
    "#     img.save('../tmp/train_images/'+f)\n",
    "# os.mkdir('../tmp/test_images/')\n",
    "# for i,f in enumerate(TEST_IMG):\n",
    "#     img = Image.open('../input/steel-surface-defect-detection/test_images/'+f)\n",
    "#     img.save('../tmp/test_images/'+f)\n",
    "\n",
    "# # %% [code]\n",
    "# img_dir = '../tmp/'\n",
    "# img_height = 256; img_width = 256\n",
    "# batch_size = 32; nb_epochs = 15\n",
    "\n",
    "# train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "#     horizontal_flip=True,\n",
    "#     validation_split=0.2) # set validation split\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#     img_dir,\n",
    "#     target_size=(img_height, img_width),\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='binary',\n",
    "#     subset='training') # set as training data\n",
    "\n",
    "# validation_generator = train_datagen.flow_from_directory(\n",
    "#     img_dir, # same directory as training data\n",
    "#     target_size=(img_height, img_width),\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='binary',\n",
    "#     subset='validation') # set as validation data\n",
    "\n",
    "# %% [code]\n",
    "import math\n",
    "\n",
    "# %% [code]\n",
    "def combine_images(generated_images): #for visualizing\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:4]\n",
    "    image = np.zeros((height*shape[0], width*shape[1], shape[2]), dtype = generated_images.dtype)\n",
    "    \n",
    "    for index, image in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1],:] = img[:, :, :]\n",
    "    return image\n",
    "\n",
    "# %% [code]\n",
    "def generator_model():\n",
    "    inputs = Input((10,))\n",
    "    fc1 = Dense(input_dim=10, units=128*7*7)(inputs)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = LeakyReLU(0.2)(fc1)\n",
    "    fc2 = Reshape((7, 7, 128), input_shape=(128*7*7,))(fc1)\n",
    "    up1 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(fc2)\n",
    "    conv1 = Conv2D(64, (3, 3), padding='same')(up1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    up2 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv1)\n",
    "    conv2 = Conv2D(1, (5, 5), padding='same')(up2)\n",
    "    outputs = Activation('tanh')(conv2)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# %% [code]\n",
    "def discriminator_model():\n",
    "    inputs = Input((28, 28, 1))\n",
    "    conv1 = Conv2D(64, (5, 5), padding='same')(inputs)\n",
    "    conv1 = LeakyReLU(0.2)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, (5, 5), padding='same')(pool1)\n",
    "    conv2 = LeakyReLU(0.2)(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    fc1 = Flatten()(pool2)\n",
    "    fc1 = Dense(1)(fc1)\n",
    "    outputs = Activation('sigmoid')(fc1)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# %% [code]\n",
    "#we'll train discriminator on generator model(for training generator)\n",
    "\n",
    "def disc_on_gen(g, d):\n",
    "    d.trainable = False\n",
    "    gan_input = Input((10,))\n",
    "    x = g(gan_input)\n",
    "    gan_output = d(x)\n",
    "    gan = Model(inputs = gan_input, outputs = gan_output)\n",
    "    return gan\n",
    "    \n",
    "\n",
    "# %% [code]\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import initializers\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "# %% [code]\n",
    "def load_model():\n",
    "    d = discriminator_model()\n",
    "    g = generator_model()\n",
    "    d_optim = RMSprop()\n",
    "    g_optim = RMSprop(lr = 0.0002)\n",
    "    g.compile(loss = 'binary_crossentropy', optimizer = g_optim)\n",
    "    d.compile(loss = 'binary_crossentropy', optimizer = d_optim)\n",
    "    g.load_weights('./weights/discriminator.h5')\n",
    "    d.load_weights('./weights/generator.h5')\n",
    "    return g, d\n",
    "\n",
    "# %% [code]\n",
    "def train(BATCH_SIZE, X_train):\n",
    "    \n",
    "    ### model define\n",
    "    d = discriminator_model()\n",
    "    g = generator_model()\n",
    "    d_on_g = disc_on_gen(g, d)\n",
    "    d_optim = RMSprop(lr=0.0004)\n",
    "    g_optim = RMSprop(lr=0.0002)\n",
    "    g.compile(loss='mse', optimizer=g_optim)\n",
    "    d_on_g.compile(loss='mse', optimizer=g_optim)\n",
    "    d.trainable = True\n",
    "    d.compile(loss='mse', optimizer=d_optim)\n",
    "    \n",
    "\n",
    "    for epoch in range(10):\n",
    "        print (\"Epoch is\", epoch)\n",
    "        n_iter = int(X_train.shape[0]/BATCH_SIZE)\n",
    "        progress_bar = Progbar(target=n_iter)\n",
    "        \n",
    "        for index in range(n_iter):\n",
    "            # create random noise -> U(0,1) 10 latent vectors\n",
    "            noise = np.random.uniform(0, 1, size=(BATCH_SIZE, 10))\n",
    "\n",
    "            # load real data & generate fake data\n",
    "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "            generated_images = g.predict(noise, verbose=0)\n",
    "            \n",
    "            # visualize training results\n",
    "            if index % 20 == 0:\n",
    "                image = combine_images(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                cv2.imwrite('./result/'+str(epoch)+\"_\"+str(index)+\".png\", image)\n",
    "\n",
    "            # attach label for training discriminator\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            y = np.array([1] * BATCH_SIZE + [0] * BATCH_SIZE)\n",
    "            \n",
    "            # training discriminator\n",
    "            d_loss = d.train_on_batch(X, y)\n",
    "\n",
    "            # training generator\n",
    "            d.trainable = False\n",
    "            g_loss = d_on_g.train_on_batch(noise, np.array([1] * BATCH_SIZE))\n",
    "            d.trainable = True\n",
    "\n",
    "            progress_bar.update(index, values=[('g',g_loss), ('d',d_loss)])\n",
    "        print ('')\n",
    "\n",
    "        # save weights for each epoch\n",
    "        g.save_weights('weights/generator.h5', True)\n",
    "        d.save_weights('weights/discriminator.h5', True)\n",
    "    return g, d\n",
    "\n",
    "# %% [code]\n",
    "def generate(BATCH_SIZE):  #generating images\n",
    "    g = generator_model()\n",
    "    g.load_weights('./weights/generator.h5')\n",
    "    noise = np.random.uniform(0, 1, (BATCH_SIZE, 10))\n",
    "    generated_images = g.predict(noise)\n",
    "    return generated_images\n",
    "\n",
    "# %% [code]\n",
    "#anomaly loss function\n",
    "def sum_of_residual(y_true, y_pred):\n",
    "    return K.sum(K.abs(y_true - y_pred))\n",
    "\n",
    "# %% [code]\n",
    "#discriminator intermediate layer features extraction\n",
    "def feature_extractor(d = None):\n",
    "    if d is None:\n",
    "        d = discriminator_model()\n",
    "        d.load_weights('./weights/discriminator.h5')\n",
    "    intermediate_model = Model(inputs=d.layers[0].input, outputs=d.layers[-7].output)\n",
    "    intermediate_model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "    return intermediate_model\n",
    "\n",
    "# %% [code]\n",
    "def anomaly_detector(g=None, d=None):    #defining anomaly detector\n",
    "    if g is None:\n",
    "        g = generator_model()\n",
    "        g.load_weights('weights/generator.h5')\n",
    "    intermediate_model = feature_extractor(d)\n",
    "    intermediate_model.trainable = False\n",
    "    g = Model(inputs=g.layers[1].input, outputs=g.layers[-1].output)\n",
    "    g.trainable = False\n",
    "    \n",
    "    # Input layer can't be trained. Add new layer as same size & same distribution\n",
    "    \n",
    "    aInput = Input(shape=(10,))\n",
    "    gInput = Dense((10), trainable=True)(aInput)\n",
    "    gInput = Activation('sigmoid')(gInput)\n",
    "    \n",
    "    # G & D feature\n",
    "    G_out = g(gInput)\n",
    "    D_out= intermediate_model(G_out)    \n",
    "    model = Model(inputs=aInput, outputs=[G_out, D_out])\n",
    "    model.compile(loss=sum_of_residual, loss_weights= [0.90, 0.10], optimizer='rmsprop')\n",
    "    \n",
    "    # batchnorm learning phase fixed (test) : make non trainable\n",
    "    K.set_learning_phase(0)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# %% [code]\n",
    "# anomaly detection\n",
    "def compute_anomaly_score(model, x, iterations=500, d=None):\n",
    "    z = np.random.uniform(0, 1, size=(1, 10))\n",
    "    \n",
    "    intermediate_model = feature_extractor(d)\n",
    "    d_x = intermediate_model.predict(x)\n",
    "\n",
    "    # learning for changing latent\n",
    "    loss = model.fit(z, [x, d_x], batch_size=1, epochs=iterations, verbose=0)\n",
    "    similar_data, _ = model.predict(z)\n",
    "    \n",
    "    loss = loss.history['loss'][-1]\n",
    "    \n",
    "    return loss, similar_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6774b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f966b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17084b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad60ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b701693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94b162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754bede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
