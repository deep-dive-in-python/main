{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e4936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Pictures\\ANACONDA\\envs\\tensorflow\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d21767e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ab5af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "LR = 1e-2\n",
    "BS = 16384\n",
    "SEED = 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd67825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "097c39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('C:/Users/김승우/Desktop/파이썬/train.csv')\n",
    "train_df = train_df.drop(columns=['ID'])\n",
    "val_df = pd.read_csv('C:/Users/김승우/Desktop/파이썬/val.csv')\n",
    "val_df = val_df.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52df81f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, eval_mode):\n",
    "        self.df = df\n",
    "        self.eval_mode = eval_mode\n",
    "        if self.eval_mode:\n",
    "            self.labels = self.df['Class'].values\n",
    "            self.df = self.df.drop(columns=['Class']).values\n",
    "        else:\n",
    "            self.df = self.df.values\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.eval_mode:\n",
    "            self.x = self.df[index]\n",
    "            self.y = self.labels[index]\n",
    "            return torch.Tensor(self.x), self.y\n",
    "        else:\n",
    "            self.x = self.df[index]\n",
    "            return torch.Tensor(self.x)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76cbe61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(df=train_df, eval_mode=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = MyDataset(df = val_df, eval_mode=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BS, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04e97d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.Encoder = nn.Sequential(\n",
    "            nn.Linear(30,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.Decoder = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64,30),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.Encoder(x)\n",
    "        x = self.Decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a46e5485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.Encoder = nn.Sequential(\n",
    "            nn.Linear(30,15),\n",
    "            nn.BatchNorm1d(15),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(15,3),\n",
    "            nn.BatchNorm1d(3),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.Decoder = nn.Sequential(\n",
    "            nn.Linear(3,15),\n",
    "            nn.BatchNorm1d(15),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(15,30),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.Encoder(x)\n",
    "        x = self.Decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cae9d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.Encoder = nn.Sequential(\n",
    "            nn.Linear(30,20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(20,10),\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.Decoder = nn.Sequential(\n",
    "            nn.Linear(10,20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(20,30),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.Encoder(x)\n",
    "        x = self.Decoder(x)\n",
    "        return x\n",
    "\n",
    "#29퍼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "970ab1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.Encoder = nn.Sequential(\n",
    "            nn.Linear(30,60),\n",
    "            nn.BatchNorm1d(60),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(60,120),\n",
    "            nn.BatchNorm1d(120),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(120,240),\n",
    "            nn.BatchNorm1d(240),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.Decoder = nn.Sequential(\n",
    "            nn.Linear(240,120),\n",
    "            nn.BatchNorm1d(120),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(120,60),\n",
    "            nn.BatchNorm1d(60),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(60,30)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.Encoder(x)\n",
    "        x = self.Decoder(x)\n",
    "        return x\n",
    "    \n",
    "# 77퍼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2b8d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.Encoder = nn.Sequential(\n",
    "            nn.Linear(30,40),\n",
    "            nn.BatchNorm1d(40),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(40,50),\n",
    "            nn.BatchNorm1d(50),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(50,60),\n",
    "            nn.BatchNorm1d(60),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.Decoder = nn.Sequential(\n",
    "            nn.Linear(60,50),\n",
    "            nn.BatchNorm1d(50),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(50,40),\n",
    "            nn.BatchNorm1d(40),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(40,30)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.Encoder(x)\n",
    "        x = self.Decoder(x)\n",
    "        return x\n",
    "    \n",
    "#50퍼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2ca8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        # Loss Function\n",
    "        self.criterion = nn.L1Loss().to(self.device)\n",
    "        \n",
    "    def fit(self, ):\n",
    "        self.model.to(self.device)\n",
    "        best_score = 0\n",
    "        for epoch in range(EPOCHS):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            for x in iter(self.train_loader):\n",
    "                x = x.float().to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                _x = self.model(x)\n",
    "                loss = self.criterion(x, _x)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            score = self.validation(self.model, 0.95)\n",
    "            print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n",
    "\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(score)\n",
    "\n",
    "            if best_score < score:\n",
    "                best_score = score\n",
    "                torch.save(model.module.state_dict(), './best_model.pth', _use_new_zipfile_serialization=False)\n",
    "    \n",
    "    def validation(self, eval_model, thr):\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        eval_model.eval()\n",
    "        pred = []\n",
    "        true = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in iter(self.val_loader):\n",
    "                x = x.float().to(self.device)\n",
    "\n",
    "                _x = self.model(x)\n",
    "                diff = cos(x, _x).cpu().tolist()\n",
    "                batch_pred = np.where(np.array(diff)<thr, 1,0).tolist()\n",
    "                pred += batch_pred\n",
    "                true += y.tolist()\n",
    "\n",
    "        return f1_score(true, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "600091ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [0] Train loss : [0.627136835030147] Val Score : [0.0010529271374420891])\n",
      "Epoch : [1] Train loss : [0.5143784710339138] Val Score : [0.0014752523784167275])\n",
      "Epoch : [2] Train loss : [0.44464688641684397] Val Score : [0.0036866388447829718])\n",
      "Epoch : [3] Train loss : [0.39273429768426077] Val Score : [0.0300469750844658])\n",
      "Epoch : [4] Train loss : [0.346460155078343] Val Score : [0.04674926570980915])\n",
      "Epoch : [5] Train loss : [0.3074759244918823] Val Score : [0.08154858949951575])\n",
      "Epoch : [6] Train loss : [0.2792639264038631] Val Score : [0.12313211409590935])\n",
      "Epoch : [7] Train loss : [0.25706655638558523] Val Score : [0.1967026311655492])\n",
      "Epoch : [8] Train loss : [0.24170141347817012] Val Score : [0.22882626241160542])\n",
      "Epoch : [9] Train loss : [0.2285268349306924] Val Score : [0.28072243527164215])\n",
      "Epoch : [10] Train loss : [0.21754797654492514] Val Score : [0.3114867275878908])\n",
      "Epoch : [11] Train loss : [0.2074135967663356] Val Score : [0.3445639369092003])\n",
      "Epoch : [12] Train loss : [0.19842598055090224] Val Score : [0.3559430647842963])\n",
      "Epoch : [13] Train loss : [0.19370416019644057] Val Score : [0.38746814756470566])\n",
      "Epoch : [14] Train loss : [0.1858151342187609] Val Score : [0.3953978545836099])\n",
      "Epoch : [15] Train loss : [0.17894227802753448] Val Score : [0.40400309909790977])\n",
      "Epoch : [16] Train loss : [0.17549404289041245] Val Score : [0.4176355763963437])\n",
      "Epoch : [17] Train loss : [0.16993593956742967] Val Score : [0.41372754057863403])\n",
      "Epoch : [18] Train loss : [0.1670903925384794] Val Score : [0.42474971191880134])\n",
      "Epoch : [19] Train loss : [0.1664567185299737] Val Score : [0.42989437789987633])\n",
      "Epoch : [20] Train loss : [0.16426399988787516] Val Score : [0.43848528508504836])\n",
      "Epoch : [21] Train loss : [0.1568839337144579] Val Score : [0.44742335142129147])\n",
      "Epoch : [22] Train loss : [0.15178387292793818] Val Score : [0.45865061579971694])\n",
      "Epoch : [23] Train loss : [0.1476162544318608] Val Score : [0.4610536921748671])\n",
      "Epoch : [24] Train loss : [0.14408839813300542] Val Score : [0.4591356783250313])\n",
      "Epoch : [25] Train loss : [0.14572909687246596] Val Score : [0.470258834938998])\n",
      "Epoch : [26] Train loss : [0.14575102712426866] Val Score : [0.4706944335010808])\n",
      "Epoch : [27] Train loss : [0.140254835997309] Val Score : [0.47787047925696396])\n",
      "Epoch : [28] Train loss : [0.13698737536157882] Val Score : [0.4796693599249994])\n",
      "Epoch : [29] Train loss : [0.1349494137934276] Val Score : [0.4838469561548474])\n",
      "Epoch : [30] Train loss : [0.13210568044866836] Val Score : [0.48431427859802983])\n",
      "Epoch : [31] Train loss : [0.12925175896712712] Val Score : [0.48955191919871194])\n",
      "Epoch : [32] Train loss : [0.12765139128480638] Val Score : [0.49368006932816727])\n",
      "Epoch : [33] Train loss : [0.12459918217999595] Val Score : [0.49262226691504296])\n",
      "Epoch : [34] Train loss : [0.1263148050223078] Val Score : [0.4877641364550968])\n",
      "Epoch : [35] Train loss : [0.12318373577935356] Val Score : [0.4947952542172809])\n",
      "Epoch : [36] Train loss : [0.11972342431545258] Val Score : [0.49637969781274394])\n",
      "Epoch : [37] Train loss : [0.11850253918341228] Val Score : [0.4939254015992857])\n",
      "Epoch : [38] Train loss : [0.11516375520399638] Val Score : [0.4971385191567011])\n",
      "Epoch : [39] Train loss : [0.11112341071878161] Val Score : [0.4958605941185105])\n",
      "Epoch : [40] Train loss : [0.11162516155413219] Val Score : [0.498531589720329])\n",
      "Epoch : [41] Train loss : [0.11034602458987917] Val Score : [0.4971718640963656])\n",
      "Epoch : [42] Train loss : [0.10988794692925044] Val Score : [0.4939869506876514])\n",
      "Epoch : [43] Train loss : [0.11229314655065536] Val Score : [0.498932461914547])\n",
      "Epoch : [44] Train loss : [0.10877947722162519] Val Score : [0.4989149346596819])\n",
      "Epoch : [45] Train loss : [0.10657212031739098] Val Score : [0.49953387441310304])\n",
      "Epoch : [46] Train loss : [0.10733882124934878] Val Score : [0.49738934757124276])\n",
      "Epoch : [47] Train loss : [0.10688455296414238] Val Score : [0.49742292172938085])\n",
      "Epoch : [48] Train loss : [0.10676737768309456] Val Score : [0.49861833883575407])\n",
      "Epoch : [49] Train loss : [0.10577507210629326] Val Score : [0.49637969781274394])\n",
      "Epoch : [50] Train loss : [0.10325252690485545] Val Score : [0.4991611446919345])\n",
      "Epoch : [51] Train loss : [0.09840009255068642] Val Score : [0.4987924956204243])\n",
      "Epoch : [52] Train loss : [0.10059527733496257] Val Score : [0.4989675435575107])\n",
      "Epoch : [53] Train loss : [0.10133761167526245] Val Score : [0.501597562474595])\n",
      "Epoch : [54] Train loss : [0.10138206503220967] Val Score : [0.500585846570022])\n",
      "Epoch : [55] Train loss : [0.10113097301551274] Val Score : [0.4941103118805053])\n",
      "Epoch : [56] Train loss : [0.09636618516274861] Val Score : [0.5019038090603355])\n",
      "Epoch : [57] Train loss : [0.0986784068601472] Val Score : [0.5007707516955575])\n",
      "Epoch : [58] Train loss : [0.0994665122457913] Val Score : [0.49846234672068895])\n",
      "Epoch : [59] Train loss : [0.09923477151564189] Val Score : [0.5002556988120173])\n",
      "Epoch : [60] Train loss : [0.09840222128799983] Val Score : [0.5003653702182089])\n",
      "Epoch : [61] Train loss : [0.0941745936870575] Val Score : [0.5009380973388867])\n",
      "Epoch : [62] Train loss : [0.09511133283376694] Val Score : [0.49861833883575407])\n",
      "Epoch : [63] Train loss : [0.09782911943537849] Val Score : [0.5004386933604956])\n",
      "Epoch : [64] Train loss : [0.0980300030538014] Val Score : [0.501865359116716])\n",
      "Epoch : [65] Train loss : [0.09089342398302895] Val Score : [0.5023690766906619])\n",
      "Epoch : [66] Train loss : [0.09036583346979958] Val Score : [0.5012190092285378])\n",
      "Epoch : [67] Train loss : [0.09349683140005384] Val Score : [0.5017311665537741])\n",
      "Epoch : [68] Train loss : [0.09000600235802787] Val Score : [0.5013320882783676])\n",
      "Epoch : [69] Train loss : [0.09074233259473528] Val Score : [0.5012378271033461])\n",
      "Epoch : [70] Train loss : [0.0908244497009686] Val Score : [0.5023690766906619])\n",
      "Epoch : [71] Train loss : [0.09023374851260867] Val Score : [0.5024081773027991])\n",
      "Epoch : [72] Train loss : [0.09194287657737732] Val Score : [0.5026438684338704])\n",
      "Epoch : [73] Train loss : [0.08979730414492744] Val Score : [0.49996505056199136])\n",
      "Epoch : [74] Train loss : [0.08877523775611605] Val Score : [0.5021161627118177])\n",
      "Epoch : [75] Train loss : [0.0910461310829435] Val Score : [0.5023495456120468])\n",
      "Epoch : [76] Train loss : [0.088361208992345] Val Score : [0.5026833334589753])\n",
      "Epoch : [77] Train loss : [0.0869811060173171] Val Score : [0.502841724590064])\n",
      "Epoch : [78] Train loss : [0.08714819486652102] Val Score : [0.5031811782542777])\n",
      "Epoch : [79] Train loss : [0.08763304884944643] Val Score : [0.5020194529667694])\n",
      "Epoch : [80] Train loss : [0.08634327671357564] Val Score : [0.5036877149960319])\n",
      "Epoch : [81] Train loss : [0.08755681557314736] Val Score : [0.5029810217524325])\n",
      "Epoch : [82] Train loss : [0.08715635963848659] Val Score : [0.5022520814854959])\n",
      "Epoch : [83] Train loss : [0.09033782247986112] Val Score : [0.501445582554562])\n",
      "Epoch : [84] Train loss : [0.09358409153563636] Val Score : [0.5017886051269973])\n",
      "Epoch : [85] Train loss : [0.08862602817160743] Val Score : [0.501445582554562])\n",
      "Epoch : [86] Train loss : [0.0863540672830173] Val Score : [0.5020967958871082])\n",
      "Epoch : [87] Train loss : [0.08438887234245028] Val Score : [0.5029013422188474])\n",
      "Epoch : [88] Train loss : [0.08458320902926582] Val Score : [0.501865359116716])\n",
      "Epoch : [89] Train loss : [0.08591949407543455] Val Score : [0.5027426302336593])\n",
      "Epoch : [90] Train loss : [0.08427689756665911] Val Score : [0.5036672810398343])\n",
      "Epoch : [91] Train loss : [0.08575572179896492] Val Score : [0.5033423031780787])\n",
      "Epoch    92: reducing learning rate of group 0 to 5.0000e-03.\n",
      "Epoch : [92] Train loss : [0.07880936988762446] Val Score : [0.5041825891765471])\n",
      "Epoch : [93] Train loss : [0.07554827736956733] Val Score : [0.5042242224252992])\n",
      "Epoch : [94] Train loss : [0.07605530960219246] Val Score : [0.5041410174273849])\n",
      "Epoch : [95] Train loss : [0.07595437871558326] Val Score : [0.503892864198786])\n",
      "Epoch : [96] Train loss : [0.07544843852519989] Val Score : [0.5035856909160894])\n",
      "Epoch : [97] Train loss : [0.0720043373959405] Val Score : [0.5039340722728054])\n",
      "Epoch : [98] Train loss : [0.07636668958834239] Val Score : [0.5038517158629292])\n",
      "Epoch : [99] Train loss : [0.07469673774072103] Val Score : [0.5042450621923206])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [100] Train loss : [0.07311975849526269] Val Score : [0.5044963554870168])\n",
      "Epoch : [101] Train loss : [0.07374451948063714] Val Score : [0.5042033980974364])\n",
      "Epoch : [102] Train loss : [0.0737408207995551] Val Score : [0.5041410174273849])\n",
      "Epoch : [103] Train loss : [0.07344902519668851] Val Score : [0.5041202545351772])\n",
      "Epoch : [104] Train loss : [0.07440844923257828] Val Score : [0.5043076744515074])\n",
      "Epoch : [105] Train loss : [0.07369480282068253] Val Score : [0.5044753279074133])\n",
      "Epoch : [106] Train loss : [0.0739289043205125] Val Score : [0.5046439946618249])\n",
      "Epoch : [107] Train loss : [0.07156032643147878] Val Score : [0.5041202545351772])\n",
      "Epoch : [108] Train loss : [0.07260386858667646] Val Score : [0.5048562793580369])\n",
      "Epoch : [109] Train loss : [0.07270460575819016] Val Score : [0.5048989325478999])\n",
      "Epoch : [110] Train loss : [0.0745554119348526] Val Score : [0.5047924228377401])\n",
      "Epoch : [111] Train loss : [0.0764068141579628] Val Score : [0.5048562793580369])\n",
      "Epoch : [112] Train loss : [0.0739467026931899] Val Score : [0.5050916935896458])\n",
      "Epoch : [113] Train loss : [0.0725117272564343] Val Score : [0.5049416518061809])\n",
      "Epoch : [114] Train loss : [0.07401448168924876] Val Score : [0.5051993707152821])\n",
      "Epoch : [115] Train loss : [0.07022923550435475] Val Score : [0.5052209570532322])\n",
      "Epoch : [116] Train loss : [0.07071354240179062] Val Score : [0.5049416518061809])\n",
      "Epoch : [117] Train loss : [0.06821297428437642] Val Score : [0.5051347136727619])\n",
      "Epoch : [118] Train loss : [0.0684948159115655] Val Score : [0.5048349774509568])\n",
      "Epoch : [119] Train loss : [0.07114564733845848] Val Score : [0.504963036299151])\n",
      "Epoch : [120] Train loss : [0.0733882412314415] Val Score : [0.5049844374152298])\n",
      "Epoch : [121] Train loss : [0.06840656804186958] Val Score : [0.5052425604474049])\n",
      "Epoch : [122] Train loss : [0.06802607966320855] Val Score : [0.5049844374152298])\n",
      "Epoch : [123] Train loss : [0.0691785141825676] Val Score : [0.5053074733353266])\n",
      "Epoch : [124] Train loss : [0.06906943448952266] Val Score : [0.5048989325478999])\n",
      "Epoch : [125] Train loss : [0.06957386434078217] Val Score : [0.5052209570532322])\n",
      "Epoch : [126] Train loss : [0.06775146403482982] Val Score : [0.5053725410586651])\n",
      "Epoch : [127] Train loss : [0.06669210003955024] Val Score : [0.5048349774509568])\n",
      "Epoch : [128] Train loss : [0.06994320345776421] Val Score : [0.5050058551899574])\n",
      "Epoch : [129] Train loss : [0.06875098922422954] Val Score : [0.5049202839008781])\n",
      "Epoch : [130] Train loss : [0.0670802891254425] Val Score : [0.5053291453229769])\n",
      "Epoch : [131] Train loss : [0.06739506338323865] Val Score : [0.5048775977120017])\n",
      "Epoch : [132] Train loss : [0.06646256893873215] Val Score : [0.5052425604474049])\n",
      "Epoch : [133] Train loss : [0.07105397965226855] Val Score : [0.5051131951942098])\n",
      "Epoch : [134] Train loss : [0.06746117770671844] Val Score : [0.5049416518061809])\n",
      "Epoch   135: reducing learning rate of group 0 to 2.5000e-03.\n",
      "Epoch : [135] Train loss : [0.06491051454629217] Val Score : [0.5054377646238957])\n",
      "Epoch : [136] Train loss : [0.06390537481222834] Val Score : [0.50541600605724])\n",
      "Epoch : [137] Train loss : [0.06731840861695153] Val Score : [0.5051993707152821])\n",
      "Epoch : [138] Train loss : [0.0626264326274395] Val Score : [0.5055249735538976])\n",
      "Epoch : [139] Train loss : [0.060612392744847705] Val Score : [0.5054813340804454])\n",
      "Epoch : [140] Train loss : [0.06354202649423055] Val Score : [0.5054595406190284])\n",
      "Epoch : [141] Train loss : [0.06400504601853234] Val Score : [0.5052425604474049])\n",
      "Epoch : [142] Train loss : [0.06219941856605666] Val Score : [0.5055905647125774])\n",
      "Epoch : [143] Train loss : [0.06153394654393196] Val Score : [0.5053942648813612])\n",
      "Epoch : [144] Train loss : [0.061372323759964535] Val Score : [0.5053942648813612])\n",
      "Epoch : [145] Train loss : [0.06281060193266187] Val Score : [0.5054595406190284])\n",
      "Epoch : [146] Train loss : [0.061198681592941284] Val Score : [0.5051778013969025])\n",
      "Epoch : [147] Train loss : [0.06256837929998126] Val Score : [0.5052209570532322])\n",
      "Epoch : [148] Train loss : [0.06454713429723467] Val Score : [0.5049844374152298])\n",
      "Epoch : [149] Train loss : [0.06481392468724932] Val Score : [0.5053942648813612])\n",
      "Epoch : [150] Train loss : [0.06307404222232955] Val Score : [0.5049202839008781])\n",
      "Epoch : [151] Train loss : [0.062089783804757256] Val Score : [0.5053725410586651])\n",
      "Epoch : [152] Train loss : [0.06170648655721119] Val Score : [0.5053508345516644])\n",
      "Epoch : [153] Train loss : [0.06374741079551834] Val Score : [0.5051562490615447])\n",
      "Epoch   154: reducing learning rate of group 0 to 1.2500e-03.\n",
      "Epoch : [154] Train loss : [0.06344583258032799] Val Score : [0.5053291453229769])\n",
      "Epoch : [155] Train loss : [0.059459281287023] Val Score : [0.50541600605724])\n",
      "Epoch : [156] Train loss : [0.05778389051556587] Val Score : [0.5055249735538976])\n",
      "Epoch : [157] Train loss : [0.05814577106918607] Val Score : [0.5053725410586651])\n",
      "Epoch : [158] Train loss : [0.05743257754615375] Val Score : [0.5053942648813612])\n",
      "Epoch : [159] Train loss : [0.05809801710503442] Val Score : [0.5055031450460611])\n",
      "Epoch : [160] Train loss : [0.058764618422303884] Val Score : [0.5056343805647541])\n",
      "Epoch : [161] Train loss : [0.059333083885056634] Val Score : [0.5055686833488627])\n",
      "Epoch : [162] Train loss : [0.05791591373937471] Val Score : [0.5053508345516644])\n",
      "Epoch : [163] Train loss : [0.05695117584296635] Val Score : [0.5055249735538976])\n",
      "Epoch : [164] Train loss : [0.05739930431757655] Val Score : [0.5054595406190284])\n",
      "Epoch   165: reducing learning rate of group 0 to 6.2500e-04.\n",
      "Epoch : [165] Train loss : [0.05708473548293114] Val Score : [0.5053942648813612])\n",
      "Epoch : [166] Train loss : [0.05712474722947393] Val Score : [0.5055468196420854])\n",
      "Epoch : [167] Train loss : [0.05662594309874943] Val Score : [0.5055031450460611])\n",
      "Epoch : [168] Train loss : [0.05458950038467135] Val Score : [0.5055686833488627])\n",
      "Epoch : [169] Train loss : [0.05597473840628352] Val Score : [0.5055686833488627])\n",
      "Epoch : [170] Train loss : [0.055332733052117486] Val Score : [0.5054813340804454])\n",
      "Epoch : [171] Train loss : [0.05610454721110208] Val Score : [0.5056563151304586])\n",
      "Epoch : [172] Train loss : [0.05622057829584394] Val Score : [0.5054595406190284])\n",
      "Epoch : [173] Train loss : [0.056912231126001904] Val Score : [0.5056343805647541])\n",
      "Epoch : [174] Train loss : [0.058017514646053314] Val Score : [0.5054377646238957])\n",
      "Epoch : [175] Train loss : [0.05738534991230283] Val Score : [0.5055468196420854])\n",
      "Epoch   176: reducing learning rate of group 0 to 3.1250e-04.\n",
      "Epoch : [176] Train loss : [0.05606748057263238] Val Score : [0.5055031450460611])\n",
      "Epoch : [177] Train loss : [0.056351653167179654] Val Score : [0.5055905647125774])\n",
      "Epoch : [178] Train loss : [0.05396846043212073] Val Score : [0.5054595406190284])\n",
      "Epoch : [179] Train loss : [0.05504330886261804] Val Score : [0.5055686833488627])\n",
      "Epoch : [180] Train loss : [0.055510010570287704] Val Score : [0.5055686833488627])\n",
      "Epoch : [181] Train loss : [0.0555653806243624] Val Score : [0.5055031450460611])\n",
      "Epoch : [182] Train loss : [0.05412949728114264] Val Score : [0.5055249735538976])\n",
      "Epoch : [183] Train loss : [0.055510152131319046] Val Score : [0.5055468196420854])\n",
      "Epoch : [184] Train loss : [0.054877687245607376] Val Score : [0.5055031450460611])\n",
      "Epoch : [185] Train loss : [0.054998217948845456] Val Score : [0.5055031450460611])\n",
      "Epoch : [186] Train loss : [0.05580800345965794] Val Score : [0.5055905647125774])\n",
      "Epoch   187: reducing learning rate of group 0 to 1.5625e-04.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11980\\1297854810.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11980\\1260855155.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0m_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Pictures\\ANACONDA\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Pictures\\ANACONDA\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DataParallel.forward\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Pictures\\ANACONDA\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11980\\1533701032.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Pictures\\ANACONDA\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Pictures\\ANACONDA\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Pictures\\ANACONDA\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Pictures\\ANACONDA\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 738\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Pictures\\ANACONDA\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[1;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[0;32m   1473\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1475\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1476\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = nn.DataParallel(AutoEncoder())\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
    "\n",
    "trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, device)\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84936436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): AutoEncoder(\n",
       "    (Encoder): Sequential(\n",
       "      (0): Linear(in_features=30, out_features=64, bias=True)\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (Decoder): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Linear(in_features=64, out_features=30, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoEncoder()\n",
    "model.load_state_dict(torch.load('./best_model.pth'))\n",
    "model = nn.DataParallel(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d4a5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('C:/Users/김승우/Desktop/파이썬/test.csv')\n",
    "test_df = test_df.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "230ed2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MyDataset(test_df, False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BS, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a0baa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, thr, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        for x in iter(test_loader):\n",
    "            x = x.float().to(device)\n",
    "            \n",
    "            _x = model(x)\n",
    "            \n",
    "            diff = cos(x, _x).cpu().tolist()\n",
    "            batch_pred = np.where(np.array(diff)<thr, 1,0).tolist()\n",
    "            pred += batch_pred\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e106c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = prediction(model, 0.95, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05a23228",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('C:/Users/김승우/Desktop/파이썬/sample_submission.csv')\n",
    "submit['Class'] = preds\n",
    "submit.to_csv('./submit_autoencoder.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239400ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8098c6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa9417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fee527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
